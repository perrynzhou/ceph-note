###  Ceph如何进行数据分布


| 作者 | 时间 |QQ技术交流群 |
| ------ | ------ |------ |
| perrynzhou@gmail.com |2020/12/01 |672152841 |

#### 数据分布

- 在分布式系统中数据数据分布和均衡非常重要，数据分布必须解决2个问题，第一是在系统中存储设备发生变化如何最小化迁移数据量而使得系统尽快恢复平衡；第二是在海量分布式存储中，存储策略一般采用多副本，如何合理分布这些备份从而使得数据具有较高的可靠性和冗错性。下面我们对比性下Glusterf和Ceph.Gluster采用在客户端计算文件hash和文件父目录布局来决定数据所在的后端的brick，Ceph是经过数据对象->PG，PG->OSD这两层计算最终决定数据所存储的位置。其中PG->OSD的计算过程的输入是每个OSD的布局(CRUSH)，这个是一个动态的布局，随着添加或者删除OSD而变化，当然用户可以更改这些布局。Gluster这种数据定位的规则是固定的。

### Crush算法

- CRUSH算法是一种基于哈希的数据分布算法，以数据唯一标识、当前存储集群的拓扑结构、数据存储策略作为CRUSH算法输入，然后计算出数据所在的位置(osd所对应的磁盘)并直接与其通信，避免查找，实现去中心化。目前CRUSH算法支持多副本、EC的存储策略
- CRUSH算法支持4中算法的实现，分别是unique、list、tree、straw。unique算法执行效率最高但是抵御结构变化能力最差；straw算法执行效率较低但是抵御结构变化能力最好；list和tree算法执行效率和抵御结构变化能力介于两者之间。随着数据爆炸式增长，集群的不断的扩容或者出现故障，一般straw算法是比较合理
- straw算法是将所有元素按照其当前权重进行逆序排列后逐个计算每个osd的签长，计算过程不断累积当前osd和后续osd的权重之差，以此作为计算下一个元素签长的基准，因此straw算法计算结果不断取决于元素的自身权重，而且和集合当中其他osd的权重也强相关，从而导致每次添加osd或者删除osd触发不相关的数据迁移。为此引入了straw2算法，这个算法计算签长时仅使用元素自身权重，这样不依赖于其他的osd的信息，避免不必要的数据迁移。
- crush算法是基于权重把所有数据映射到存储设备之上，这个过程是受控的并且高度依赖于 集群的拓扑图(crush map),不同的数据存储策略通过制定不同的placement rule(常规有3副本或者ec 规则)实现。比如写入数据data.txt,ceph集群先把数据按照obejct_size切分为一个或者多个object,通过object映射到不同的PG,PG是通过crush计算将映射为后端的osd中，crush的计算是以数据object、cluster map、placement rule作为哈希函数的输入，如果当前cluster map不发生变化，那么结果是确定的。crush使用的哈希函数是伪随机的，因此crush选择每个目标对象(osd)的概率相对独立，从而可以保证集群之间数据均匀分布

### Cluster Map

- cluster map是ceph集群拓扑结构的逻辑呈现形式。实际中ceph集群通常具有 数据中心->机架->主机->磁盘 这样的树状层级关系，所以cluster map是以这种数据结构来实现。每个节点都是bucket,一般叶子节点都是osd.

### 数据分布策略


####  数据对象选择osd的过程

- 使用crush map建立对应集群的拓扑结构后，可以定义placement rule来完成数据映射。每个placement rule包括多个操作，分别为take、select、emit过程
- take过程是从cluster map中选择指定编号的bucket作为后续输入，ceph默认的placement rule是以cluster map中的root节点作为输入。
- select过程是从输入的bucket当前随机挑选指定类型和数量的条目，ceph当前支持副本和ec分别对应两种select算法firstn和indep.两个算法在都是深度优先，并无显著不同，主要的差别在于EC要求结果有序。如果选择满足数量为4的的输出，如果不满足，对于firstn会输出[1,2,4];对于indep则会输出[1,2,CHRUSH_ITEM_NONE,4]，indep总返回要求数量的条目，如果对应条目不存在则以空的item填充。

- emit过程是输出最终结果给上层调用者并返回。所以在ceph集群中placement rule真正起到决定性做作用的是select操作。


#### osd的数据过载
- 哈希函数和哈希函数派生出来的crush算法理论上可以保证每个OSD上数据均匀分布，但是当集群规模较小(这里是osd数量较少)，集群的整体容量有限，导致集群PG总数有限，同时也会导致crush算法输入容量不够
- crush的缺陷，crush的基本选择算法中(straw2),每次选择都是计算单个条目(osd)被选中的独立概率，但是crush所要求的副本策略使得针对同一个输入、多个副本之间的选择变成了计算条件概率(副本必须位于不同的容灾域的osd上)，从原理上crush就无法出来好多副本模式下数据分布的问题。
- 上述因素会导致真实集群，特别是异构集群中，出现大量磁盘分布悬殊的情况，这需要对crush计算结果进行人工调整，这个调整是基于权重进行的，针对每个叶子设备(osd)除了由其基于容量计算得来的真是权重weight之外，ceph还为权重设置了一个额外的权重，成为reweight,算法正常选中一个osd后，最后还基于这个reweight对该osd进行一次过载测试，如果测试失败，则仍将拒绝选择条目。osd的reweight设置的越大(如果设置0x10000,直接被选择承载数据)，通过测试的概率越大,这个osd会被选择承载数据；如果osd reweight设置的越小，越不会被选择。在实际应用中，降低承载数据多的osd的reweight,在新增加osd时候，增大(比如设置0x10000)这个osd的reweight都可以出发数据在osd之间的迁移，从而使得整个集群的osd的数据分布均匀。
- 当osd暂时失效时候(比如磁盘被拔出一段时间)，ceph将其设置为out,可以通过调整这个osd的reweight为0从而利用过载测试将这个osd从候选osd中淘汰出去，这个osd的pg数据会迁移到其他的osd上，当这个osd恢复正常时候,将reweight设置为0x10000可以将原来归属该osd的数据再次迁回，这个是没有改变cluster map,所以基于straw2算法后结果是没有改变，这个有点像原地迁移。
  ```
  // osd-id是osd的编号
  // reweight必须是浮点数，取值范围在[0,1]之间，reweight越小，越多的数据从这个osd迁移到其他的osd中
  ceph osd reweight {osd_id} {reweight}

  ```
- 如果从集群中删除一个osd时候，ceph集群会将该osd从对应bucket条目中删除，这样即使该osd后来被重新添加回集群，因为在cluster map中的发生了变化(osd编号改变告知crush选择结果发生变化),所以有可能这个重新回归的osd承载的数据和之前是完全不同的数据。