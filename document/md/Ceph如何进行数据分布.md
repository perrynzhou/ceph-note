###  Ceph如何进行数据分布(持续更新)


| 作者 | 时间 |QQ技术交流群 |
| ------ | ------ |------ |
| perrynzhou@gmail.com |2020/12/01 |672152841 |

#### 数据分布

- 在分布式系统中数据数据分布和均衡非常重要，数据分布必须解决2个问题，第一是在系统中存储设备发生变化如何最小化迁移数据量而使得系统尽快恢复平衡；第二是在海量分布式存储中，存储策略一般采用多副本，如何合理分布这些备份从而使得数据具有较高的可靠性和冗错性。下面我们对比性下Glusterf和Ceph.Gluster采用在客户端计算文件hash和文件父目录布局来决定数据所在的后端的brick，Ceph是经过数据对象->PG，PG->OSD这两层计算最终决定数据所存储的位置。其中PG->OSD的计算过程的输入是每个OSD的布局(CRUSH)，这个是一个动态的布局，随着添加或者删除OSD而变化，当然用户可以更改这些布局。Gluster这种数据定位的规则是固定的。

### Crush算法

- CRUSH算法是一种基于哈希的数据分布算法，以数据唯一标识、当前存储集群的拓扑结构、数据存储策略作为CRUSH算法输入，然后计算出数据所在的位置(osd所对应的磁盘)并直接与其通信，避免查找，实现去中心化。目前CRUSH算法支持多副本、EC的存储策略
- CRUSH算法支持4中算法的实现，分别是unique、list、tree、straw。unique算法执行效率最高但是抵御结构变化能力最差；straw算法执行效率较低但是抵御结构变化能力最好；list和tree算法执行效率和抵御结构变化能力介于两者之间。随着数据爆炸式增长，集群的不断的扩容或者出现故障，一般straw算法是比较合理
- straw算法是将所有元素按照其当前权重进行逆序排列后逐个计算每个osd的签长，计算过程不断累积当前osd和后续osd的权重之差，以此作为计算下一个元素签长的基准，因此straw算法计算结果不断取决于元素的自身权重，而且和集合当中其他osd的权重也强相关，从而导致每次添加osd或者删除osd触发不相关的数据迁移。为此引入了straw2算法，这个算法计算签长时仅使用元素自身权重，这样不依赖于其他的osd的信息，避免不必要的数据迁移。
- crush算法是基于权重把所有数据映射到存储设备之上，这个过程是受控的并且高度依赖于 集群的拓扑图(crush map),不同的数据存储策略通过制定不同的placement rule(常规有3副本或者ec 规则)实现。比如写入数据data.txt,ceph集群先把数据按照obejct_size切分为一个或者多个object,通过object映射到不同的PG,PG是通过crush计算将映射为后端的osd中，crush的计算是以数据object、cluster map、placement rule作为哈希函数的输入，如果当前cluster map不发生变化，那么结果是确定的。crush使用的哈希函数是伪随机的，因此crush选择每个目标对象(osd)的概率相对独立，从而可以保证集群之间数据均匀分布

### Cluster Map

- cluster map是ceph集群拓扑结构的逻辑呈现形式。实际中ceph集群通常具有 数据中心->机架->主机->磁盘 这样的树状层级关系，所以cluster map是以这种数据结构来实现。每个节点都是bucket,一般叶子节点都是osd.

### 数据分布策略

- 使用crush map建立对应集群的拓扑结构后，可以定义placement rule来完成数据映射。每个placement rule包括多个操作，分别为take、select、emit过程
- take过程是从cluster map中选择指定编号的bucket作为后续输入，ceph默认的placement rule是以cluster map中的root节点作为输入。
- select过程是从输入的bucket当前随机挑选指定类型和数量的条目，ceph当前支持副本和ec分别对应两种select算法firstn和indep.两个算法在都是深度优先，并无显著不同，主要的差别在于EC要求结果有序。如果选择满足数量为4的的输出，如果不满足，对于firstn会输出[1,2,4];对于indep则会输出[1,2,CHRUSH_ITEM_NONE,4]，indep总返回要求数量的条目，如果对应条目不存在则以空的item填充。

- emit过程是输出最终结果给上层调用者并返回。所以在ceph集群中placement rule真正起到决定性做作用的是select操作。